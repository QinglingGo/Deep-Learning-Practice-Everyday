{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cifar10_input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最大池化\n",
    "def max_pool_with_argmax(net, stride):\n",
    "    _, mask = tf.nn.max_pool_with_argmax(net,ksize=[1, stride, stride, 1], strides=[1, stride, stride, 1],padding='SAME')\n",
    "    mask = tf.stop_gradient(mask)\n",
    "    net = tf.nn.max_pool(net, ksize=[1, stride, stride, 1],strides=[1, stride, stride, 1], padding='SAME') \n",
    "    return net, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpool(net, mask, stride):\n",
    "    ksize = [1, stride, stride, 1]\n",
    "    input_shape = net.get_shape().as_list()\n",
    "\n",
    "    output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n",
    "\n",
    "    one_like_mask = tf.ones_like(mask)\n",
    "    batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int64), shape=[input_shape[0], 1, 1, 1])\n",
    "    b = one_like_mask * batch_range\n",
    "    y = mask // (output_shape[2] * output_shape[3])\n",
    "    x = mask % (output_shape[2] * output_shape[3]) // output_shape[3]\n",
    "    feature_range = tf.range(output_shape[3], dtype=tf.int64)\n",
    "    f = one_like_mask * feature_range\n",
    "\n",
    "    updates_size = tf.size(net)\n",
    "    indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\n",
    "    values = tf.reshape(net, [updates_size])\n",
    "    ret = tf.scatter_nd(indices, values, output_shape)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n",
      "WARNING:tensorflow:From /home/hu/Documents/OneDrive/CurrentProject/DLandTF/JupyterNotebook/cifar10/cifar10_input.py:232: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/hu/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/hu/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /home/hu/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/hu/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/hu/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/training/input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/hu/Documents/OneDrive/CurrentProject/DLandTF/JupyterNotebook/cifar10/cifar10_input.py:79: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "WARNING:tensorflow:From /home/hu/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/hu/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/ops/image_ops_impl.py:1241: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /home/hu/Documents/OneDrive/CurrentProject/DLandTF/JupyterNotebook/cifar10/cifar10_input.py:132: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "begin data\n"
     ]
    }
   ],
   "source": [
    "#取数据\n",
    "batch_size = 128\n",
    "workPath = os.getcwd()\n",
    "data_dir = 'tmp/cifar10_data/cifar-10-batches-bin'\n",
    "dest_directory = os.path.join(workPath, data_dir)\n",
    "print(\"begin\")\n",
    "images_train, labels_train = cifar10_input.inputs(eval_data = False,data_dir = dest_directory, batch_size = batch_size)\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data = True, data_dir = dest_directory, batch_size = batch_size)\n",
    "print(\"begin data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "  \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')  \n",
    "                        \n",
    "def avg_pool_6x6(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 6, 6, 1],\n",
    "                        strides=[1, 6, 6, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 6, 6, 64)\n",
      "(128, 12, 12, 64) (128, 12, 12, 64) (128, 12, 12, 64)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [batch_size, 24,24,3]) # cifar data image of shape 24*24*3\n",
    "y = tf.placeholder(tf.float32, [batch_size, 10]) # 0-9 数字=> 10 classes\n",
    "\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 3, 64])\n",
    "b_conv1 = bias_variable([64])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,24,24,3])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "#h_pool1 = max_pool_2x2(h_conv1)\n",
    "h_pool1, mask1 = max_pool_with_argmax(h_conv1, 2)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 64, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "#h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "#############################################################\n",
    "h_pool2, mask = max_pool_with_argmax(h_conv2, 2)#(128, 6, 6, 64)\n",
    "print(h_pool2.shape)\n",
    "t_conv2 = unpool(h_pool2, mask, 2)#(128, 12, 12, 64)\n",
    "t_pool1 = tf.nn.conv2d_transpose(t_conv2-b_conv2, W_conv2, h_pool1.shape,[1,1,1,1])#(128, 24, 24, 64)\n",
    "print(t_conv2.shape,h_pool1.shape,t_pool1.shape)\n",
    "t_conv1 = unpool(t_pool1, mask1, 2)\n",
    "t_x_image = tf.nn.conv2d_transpose(t_conv1-b_conv1, W_conv1, x_image.shape,[1,1,1,1])\n",
    "#第一层卷积还原\n",
    "t1_conv1 = unpool(h_pool1, mask1, 2)\n",
    "t1_x_image = tf.nn.conv2d_transpose(t1_conv1-b_conv1, W_conv1, x_image.shape,[1,1,1,1])\n",
    "\n",
    "# 生成最终图像\n",
    "stitched_decodings = tf.concat((x_image, t1_x_image,t_x_image), axis=2)\n",
    "decoding_summary_op = tf.summary.image('source/cifar', stitched_decodings)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "W_conv3 = weight_variable([5, 5, 64, 10])\n",
    "b_conv3 = bias_variable([10])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "\n",
    "nt_hpool3=avg_pool_6x6(h_conv3)#10\n",
    "nt_hpool3_flat = tf.reshape(nt_hpool3, [-1, 10])\n",
    "y_conv=tf.nn.softmax(nt_hpool3_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(y*tf.log(y_conv)) +(tf.nn.l2_loss(W_conv1)+tf.nn.l2_loss(W_conv2)+tf.nn.l2_loss(W_conv3))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-4781c6d90912>:5: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "step 0, training accuracy 0.132812\n",
      "cross_entropy 299.36572\n",
      "step 200, training accuracy 0.289062\n",
      "cross_entropy 240.62248\n",
      "step 400, training accuracy 0.382812\n",
      "cross_entropy 221.73228\n",
      "step 600, training accuracy 0.304688\n",
      "cross_entropy 236.3593\n",
      "step 800, training accuracy 0.429688\n",
      "cross_entropy 227.60962\n",
      "step 1000, training accuracy 0.4375\n",
      "cross_entropy 208.00002\n",
      "step 1200, training accuracy 0.414062\n",
      "cross_entropy 206.89636\n",
      "step 1400, training accuracy 0.460938\n",
      "cross_entropy 199.2229\n",
      "step 1600, training accuracy 0.398438\n",
      "cross_entropy 206.20322\n",
      "step 1800, training accuracy 0.414062\n",
      "cross_entropy 218.584\n",
      "step 2000, training accuracy 0.476562\n",
      "cross_entropy 199.63106\n",
      "step 2200, training accuracy 0.390625\n",
      "cross_entropy 222.37929\n",
      "step 2400, training accuracy 0.445312\n",
      "cross_entropy 201.31537\n",
      "step 2600, training accuracy 0.445312\n",
      "cross_entropy 218.0013\n",
      "step 2800, training accuracy 0.515625\n",
      "cross_entropy 190.11111\n",
      "step 3000, training accuracy 0.46875\n",
      "cross_entropy 195.7324\n",
      "step 3200, training accuracy 0.421875\n",
      "cross_entropy 211.50911\n",
      "step 3400, training accuracy 0.5\n",
      "cross_entropy 192.47313\n",
      "step 3600, training accuracy 0.492188\n",
      "cross_entropy 206.51616\n",
      "step 3800, training accuracy 0.554688\n",
      "cross_entropy 195.54146\n",
      "step 4000, training accuracy 0.523438\n",
      "cross_entropy 192.99818\n",
      "step 4200, training accuracy 0.539062\n",
      "cross_entropy 182.51595\n",
      "step 4400, training accuracy 0.507812\n",
      "cross_entropy 185.89252\n",
      "step 4600, training accuracy 0.515625\n",
      "cross_entropy 179.40755\n",
      "step 4800, training accuracy 0.507812\n",
      "cross_entropy 185.43211\n",
      "step 5000, training accuracy 0.554688\n",
      "cross_entropy 167.48494\n",
      "step 5200, training accuracy 0.484375\n",
      "cross_entropy 183.93863\n",
      "step 5400, training accuracy 0.617188\n",
      "cross_entropy 157.25526\n",
      "step 5600, training accuracy 0.507812\n",
      "cross_entropy 194.72687\n",
      "step 5800, training accuracy 0.585938\n",
      "cross_entropy 165.56555\n",
      "step 6000, training accuracy 0.507812\n",
      "cross_entropy 189.61128\n",
      "step 6200, training accuracy 0.515625\n",
      "cross_entropy 189.30693\n",
      "step 6400, training accuracy 0.617188\n",
      "cross_entropy 162.0711\n",
      "step 6600, training accuracy 0.625\n",
      "cross_entropy 172.04787\n",
      "step 6800, training accuracy 0.5625\n",
      "cross_entropy 175.96565\n",
      "step 7000, training accuracy 0.554688\n",
      "cross_entropy 175.1361\n",
      "step 7200, training accuracy 0.617188\n",
      "cross_entropy 155.54976\n",
      "step 7400, training accuracy 0.546875\n",
      "cross_entropy 171.75392\n",
      "step 7600, training accuracy 0.601562\n",
      "cross_entropy 155.01299\n",
      "step 7800, training accuracy 0.546875\n",
      "cross_entropy 189.92494\n",
      "step 8000, training accuracy 0.585938\n",
      "cross_entropy 167.69482\n",
      "step 8200, training accuracy 0.460938\n",
      "cross_entropy 189.40408\n",
      "step 8400, training accuracy 0.578125\n",
      "cross_entropy 166.68169\n",
      "step 8600, training accuracy 0.570312\n",
      "cross_entropy 157.564\n",
      "step 8800, training accuracy 0.617188\n",
      "cross_entropy 173.84872\n",
      "step 9000, training accuracy 0.570312\n",
      "cross_entropy 178.57588\n",
      "step 9200, training accuracy 0.546875\n",
      "cross_entropy 168.88623\n",
      "step 9400, training accuracy 0.601562\n",
      "cross_entropy 168.0081\n",
      "step 9600, training accuracy 0.578125\n",
      "cross_entropy 152.55008\n",
      "step 9800, training accuracy 0.585938\n",
      "cross_entropy 168.03432\n",
      "step 10000, training accuracy 0.609375\n",
      "cross_entropy 165.20018\n",
      "step 10200, training accuracy 0.585938\n",
      "cross_entropy 176.42506\n",
      "step 10400, training accuracy 0.625\n",
      "cross_entropy 159.9506\n",
      "step 10600, training accuracy 0.554688\n",
      "cross_entropy 180.29596\n",
      "step 10800, training accuracy 0.59375\n",
      "cross_entropy 185.41348\n",
      "step 11000, training accuracy 0.648438\n",
      "cross_entropy 154.55379\n",
      "step 11200, training accuracy 0.695312\n",
      "cross_entropy 143.74721\n",
      "step 11400, training accuracy 0.625\n",
      "cross_entropy 155.5287\n",
      "step 11600, training accuracy 0.546875\n",
      "cross_entropy 180.16847\n",
      "step 11800, training accuracy 0.585938\n",
      "cross_entropy 164.98296\n",
      "step 12000, training accuracy 0.640625\n",
      "cross_entropy 158.7733\n",
      "step 12200, training accuracy 0.59375\n",
      "cross_entropy 168.51779\n",
      "step 12400, training accuracy 0.59375\n",
      "cross_entropy 174.59448\n",
      "step 12600, training accuracy 0.679688\n",
      "cross_entropy 141.72997\n",
      "step 12800, training accuracy 0.601562\n",
      "cross_entropy 161.19495\n",
      "step 13000, training accuracy 0.625\n",
      "cross_entropy 167.22008\n",
      "step 13200, training accuracy 0.539062\n",
      "cross_entropy 175.90501\n",
      "step 13400, training accuracy 0.640625\n",
      "cross_entropy 164.23923\n",
      "step 13600, training accuracy 0.570312\n",
      "cross_entropy 172.34622\n",
      "step 13800, training accuracy 0.617188\n",
      "cross_entropy 165.66223\n",
      "step 14000, training accuracy 0.695312\n",
      "cross_entropy 149.98123\n",
      "step 14200, training accuracy 0.625\n",
      "cross_entropy 150.93925\n",
      "step 14400, training accuracy 0.570312\n",
      "cross_entropy 168.66821\n",
      "step 14600, training accuracy 0.648438\n",
      "cross_entropy 158.76532\n",
      "step 14800, training accuracy 0.601562\n",
      "cross_entropy 161.10129\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "summary_writer = tf.summary.FileWriter('./log/', sess.graph)\n",
    "\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "for i in range(15000):#20000\n",
    "    image_batch, label_batch = sess.run([images_train, labels_train])\n",
    "    label_b = np.eye(10,dtype=float)[label_batch] #one hot\n",
    "  \n",
    "    train_step.run(feed_dict={x:image_batch, y: label_b},session=sess)\n",
    "  #_, decoding_summary = sess.run([train_step, decoding_summary_op],feed_dict={x:image_batch, y: label_b})\n",
    "  \n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:image_batch, y: label_b},session=sess)\n",
    "        print( \"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        print(\"cross_entropy\",cross_entropy.eval(feed_dict={x:image_batch, y: label_b},session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished！ test accuracy 0.585938\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "label_b = np.eye(10,dtype=float)[label_batch]#one hot\n",
    "print (\"finished！ test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "     x:image_batch, y: label_b},session=sess))\n",
    "decoding_summary = sess.run(decoding_summary_op,feed_dict={x:image_batch, y: label_b})\n",
    "summary_writer.add_summary(decoding_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
