{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "输入数据: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "输入数据打shape: (55000, 784)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\")\n",
    "\n",
    "print ('输入数据:',mnist.train.images)\n",
    "print ('输入数据打shape:',mnist.train.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab \n",
    "im = mnist.train.images[1]\n",
    "im = im.reshape(-1,28)\n",
    "pylab.imshow(im)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__定义Maxout，将其作为softmax交叉熵的输入。Maxout将激活函数变为网络选择器，具有更好的拟合效果__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_out(inputs, num_units, axis=None):\n",
    "    shape = inputs.get_shape().as_list()\n",
    "    if shape[0] is None:\n",
    "        shape[0] = -1\n",
    "    if axis is None:  # Assume that channel is the last dimension\n",
    "        axis = -1\n",
    "    num_channels = shape[axis]\n",
    "    if num_channels % num_units:\n",
    "        raise ValueError('number of features({}) is not '\n",
    "                         'a multiple of num_units({})'.format(num_channels, num_units))\n",
    "    shape[axis] = num_units\n",
    "    shape += [num_channels // num_units]\n",
    "    outputs = tf.reduce_max(tf.reshape(inputs, shape), -1, keep_dims=False)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hu/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-04528ec88b83>:13: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/hu/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/hu/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data维度 28*28=784\n",
    "y = tf.placeholder(tf.int32, [None]) # 0-9 数字=> 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.random_normal([784, 100]))\n",
    "b = tf.Variable(tf.zeros([100]))\n",
    "\n",
    "\n",
    "z= tf.matmul(x, W) + b\n",
    "#maxout = tf.reduce_max(z,axis= 1,keep_dims=True)\n",
    "\n",
    "maxout= max_out(z, 50)\n",
    "\n",
    "# Set model weights\n",
    "W2 = tf.Variable(tf.truncated_normal([50, 10], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "# 构建模型\n",
    "#pred = tf.nn.softmax(tf.matmul(maxout, W2) + b2)\n",
    "pred = tf.matmul(maxout, W2) + b2\n",
    "# 构建模型\n",
    "#pred = tf.nn.softmax(z) # Softmax分类\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=pred))\n",
    "\n",
    "#参数设置\n",
    "learning_rate = 0.04\n",
    "# 使用梯度下降优化器\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.638116116\n",
      "Epoch: 0002 cost= 0.829376921\n",
      "Epoch: 0003 cost= 0.681829615\n",
      "Epoch: 0004 cost= 0.609311657\n",
      "Epoch: 0005 cost= 0.563325766\n",
      "Epoch: 0006 cost= 0.532337264\n",
      "Epoch: 0007 cost= 0.506552150\n",
      "Epoch: 0008 cost= 0.491729430\n",
      "Epoch: 0009 cost= 0.475112045\n",
      "Epoch: 0010 cost= 0.462225179\n",
      "Epoch: 0011 cost= 0.450247716\n",
      "Epoch: 0012 cost= 0.438436228\n",
      "Epoch: 0013 cost= 0.426659220\n",
      "Epoch: 0014 cost= 0.418970493\n",
      "Epoch: 0015 cost= 0.411275560\n",
      "Epoch: 0016 cost= 0.403348403\n",
      "Epoch: 0017 cost= 0.395129296\n",
      "Epoch: 0018 cost= 0.389893712\n",
      "Epoch: 0019 cost= 0.384994946\n",
      "Epoch: 0020 cost= 0.376926592\n",
      "Epoch: 0021 cost= 0.372195826\n",
      "Epoch: 0022 cost= 0.366731452\n",
      "Epoch: 0023 cost= 0.359846365\n",
      "Epoch: 0024 cost= 0.355960840\n",
      "Epoch: 0025 cost= 0.352616763\n",
      "Epoch: 0026 cost= 0.348751331\n",
      "Epoch: 0027 cost= 0.344399760\n",
      "Epoch: 0028 cost= 0.338341662\n",
      "Epoch: 0029 cost= 0.334868446\n",
      "Epoch: 0030 cost= 0.333692991\n",
      "Epoch: 0031 cost= 0.328373009\n",
      "Epoch: 0032 cost= 0.325432734\n",
      "Epoch: 0033 cost= 0.322686797\n",
      "Epoch: 0034 cost= 0.317555429\n",
      "Epoch: 0035 cost= 0.315499984\n",
      "Epoch: 0036 cost= 0.313461207\n",
      "Epoch: 0037 cost= 0.309346221\n",
      "Epoch: 0038 cost= 0.306444966\n",
      "Epoch: 0039 cost= 0.303775351\n",
      "Epoch: 0040 cost= 0.300011391\n",
      "Epoch: 0041 cost= 0.297666278\n",
      "Epoch: 0042 cost= 0.293946472\n",
      "Epoch: 0043 cost= 0.293202220\n",
      "Epoch: 0044 cost= 0.290840796\n",
      "Epoch: 0045 cost= 0.288555111\n",
      "Epoch: 0046 cost= 0.285980021\n",
      "Epoch: 0047 cost= 0.283338340\n",
      "Epoch: 0048 cost= 0.281430660\n",
      "Epoch: 0049 cost= 0.279143025\n",
      "Epoch: 0050 cost= 0.276922228\n",
      "Epoch: 0051 cost= 0.275592943\n",
      "Epoch: 0052 cost= 0.272529508\n",
      "Epoch: 0053 cost= 0.271124949\n",
      "Epoch: 0054 cost= 0.267713505\n",
      "Epoch: 0055 cost= 0.267382967\n",
      "Epoch: 0056 cost= 0.264962935\n",
      "Epoch: 0057 cost= 0.264426021\n",
      "Epoch: 0058 cost= 0.262425506\n",
      "Epoch: 0059 cost= 0.260201030\n",
      "Epoch: 0060 cost= 0.260027582\n",
      "Epoch: 0061 cost= 0.257613678\n",
      "Epoch: 0062 cost= 0.254677447\n",
      "Epoch: 0063 cost= 0.254358147\n",
      "Epoch: 0064 cost= 0.251850362\n",
      "Epoch: 0065 cost= 0.250080219\n",
      "Epoch: 0066 cost= 0.249074970\n",
      "Epoch: 0067 cost= 0.247849675\n",
      "Epoch: 0068 cost= 0.247091062\n",
      "Epoch: 0069 cost= 0.243626640\n",
      "Epoch: 0070 cost= 0.242090611\n",
      "Epoch: 0071 cost= 0.241775173\n",
      "Epoch: 0072 cost= 0.239774870\n",
      "Epoch: 0073 cost= 0.238798554\n",
      "Epoch: 0074 cost= 0.238907929\n",
      "Epoch: 0075 cost= 0.235502387\n",
      "Epoch: 0076 cost= 0.234806500\n",
      "Epoch: 0077 cost= 0.233794874\n",
      "Epoch: 0078 cost= 0.232215969\n",
      "Epoch: 0079 cost= 0.231998395\n",
      "Epoch: 0080 cost= 0.229813569\n",
      "Epoch: 0081 cost= 0.229735703\n",
      "Epoch: 0082 cost= 0.227641954\n",
      "Epoch: 0083 cost= 0.226572205\n",
      "Epoch: 0084 cost= 0.225493547\n",
      "Epoch: 0085 cost= 0.223971140\n",
      "Epoch: 0086 cost= 0.222162563\n",
      "Epoch: 0087 cost= 0.220502142\n",
      "Epoch: 0088 cost= 0.220178426\n",
      "Epoch: 0089 cost= 0.219888688\n",
      "Epoch: 0090 cost= 0.218436617\n",
      "Epoch: 0091 cost= 0.217465914\n",
      "Epoch: 0092 cost= 0.215755323\n",
      "Epoch: 0093 cost= 0.215925240\n",
      "Epoch: 0094 cost= 0.214503590\n",
      "Epoch: 0095 cost= 0.212349968\n",
      "Epoch: 0096 cost= 0.211817075\n",
      "Epoch: 0097 cost= 0.210713106\n",
      "Epoch: 0098 cost= 0.209750583\n",
      "Epoch: 0099 cost= 0.209053752\n",
      "Epoch: 0100 cost= 0.208141141\n",
      "Epoch: 0101 cost= 0.207417229\n",
      "Epoch: 0102 cost= 0.205630196\n",
      "Epoch: 0103 cost= 0.205399074\n",
      "Epoch: 0104 cost= 0.205493962\n",
      "Epoch: 0105 cost= 0.203080780\n",
      "Epoch: 0106 cost= 0.202628574\n",
      "Epoch: 0107 cost= 0.201803799\n",
      "Epoch: 0108 cost= 0.200516569\n",
      "Epoch: 0109 cost= 0.200215705\n",
      "Epoch: 0110 cost= 0.198665170\n",
      "Epoch: 0111 cost= 0.198131900\n",
      "Epoch: 0112 cost= 0.197155319\n",
      "Epoch: 0113 cost= 0.197154481\n",
      "Epoch: 0114 cost= 0.196195619\n",
      "Epoch: 0115 cost= 0.195241522\n",
      "Epoch: 0116 cost= 0.194116561\n",
      "Epoch: 0117 cost= 0.193740752\n",
      "Epoch: 0118 cost= 0.192963398\n",
      "Epoch: 0119 cost= 0.192428680\n",
      "Epoch: 0120 cost= 0.191593152\n",
      "Epoch: 0121 cost= 0.190706771\n",
      "Epoch: 0122 cost= 0.189907797\n",
      "Epoch: 0123 cost= 0.188895340\n",
      "Epoch: 0124 cost= 0.187792691\n",
      "Epoch: 0125 cost= 0.187613936\n",
      "Epoch: 0126 cost= 0.186472660\n",
      "Epoch: 0127 cost= 0.185174269\n",
      "Epoch: 0128 cost= 0.185307759\n",
      "Epoch: 0129 cost= 0.184434676\n",
      "Epoch: 0130 cost= 0.183622529\n",
      "Epoch: 0131 cost= 0.183903732\n",
      "Epoch: 0132 cost= 0.182022095\n",
      "Epoch: 0133 cost= 0.182231772\n",
      "Epoch: 0134 cost= 0.181198359\n",
      "Epoch: 0135 cost= 0.180694323\n",
      "Epoch: 0136 cost= 0.180251717\n",
      "Epoch: 0137 cost= 0.179578979\n",
      "Epoch: 0138 cost= 0.178930519\n",
      "Epoch: 0139 cost= 0.178324958\n",
      "Epoch: 0140 cost= 0.177371811\n",
      "Epoch: 0141 cost= 0.176995461\n",
      "Epoch: 0142 cost= 0.176819098\n",
      "Epoch: 0143 cost= 0.174865882\n",
      "Epoch: 0144 cost= 0.175088586\n",
      "Epoch: 0145 cost= 0.174055765\n",
      "Epoch: 0146 cost= 0.173933594\n",
      "Epoch: 0147 cost= 0.172805067\n",
      "Epoch: 0148 cost= 0.172327028\n",
      "Epoch: 0149 cost= 0.172193047\n",
      "Epoch: 0150 cost= 0.171084655\n",
      "Epoch: 0151 cost= 0.171003857\n",
      "Epoch: 0152 cost= 0.170064666\n",
      "Epoch: 0153 cost= 0.168706077\n",
      "Epoch: 0154 cost= 0.168086012\n",
      "Epoch: 0155 cost= 0.168659355\n",
      "Epoch: 0156 cost= 0.167618895\n",
      "Epoch: 0157 cost= 0.167285407\n",
      "Epoch: 0158 cost= 0.166829583\n",
      "Epoch: 0159 cost= 0.165789234\n",
      "Epoch: 0160 cost= 0.165885496\n",
      "Epoch: 0161 cost= 0.164796473\n",
      "Epoch: 0162 cost= 0.163808117\n",
      "Epoch: 0163 cost= 0.163741769\n",
      "Epoch: 0164 cost= 0.163205533\n",
      "Epoch: 0165 cost= 0.162362235\n",
      "Epoch: 0166 cost= 0.161393122\n",
      "Epoch: 0167 cost= 0.161460517\n",
      "Epoch: 0168 cost= 0.161395420\n",
      "Epoch: 0169 cost= 0.160577724\n",
      "Epoch: 0170 cost= 0.160386521\n",
      "Epoch: 0171 cost= 0.159990751\n",
      "Epoch: 0172 cost= 0.158774280\n",
      "Epoch: 0173 cost= 0.159191676\n",
      "Epoch: 0174 cost= 0.157896634\n",
      "Epoch: 0175 cost= 0.157407054\n",
      "Epoch: 0176 cost= 0.157154502\n",
      "Epoch: 0177 cost= 0.156029532\n",
      "Epoch: 0178 cost= 0.155414236\n",
      "Epoch: 0179 cost= 0.155267293\n",
      "Epoch: 0180 cost= 0.155691407\n",
      "Epoch: 0181 cost= 0.154926005\n",
      "Epoch: 0182 cost= 0.153765585\n",
      "Epoch: 0183 cost= 0.153656174\n",
      "Epoch: 0184 cost= 0.152858698\n",
      "Epoch: 0185 cost= 0.153650462\n",
      "Epoch: 0186 cost= 0.152723936\n",
      "Epoch: 0187 cost= 0.152087158\n",
      "Epoch: 0188 cost= 0.151435220\n",
      "Epoch: 0189 cost= 0.151005737\n",
      "Epoch: 0190 cost= 0.150762512\n",
      "Epoch: 0191 cost= 0.150220397\n",
      "Epoch: 0192 cost= 0.149438522\n",
      "Epoch: 0193 cost= 0.148926094\n",
      "Epoch: 0194 cost= 0.147854494\n",
      "Epoch: 0195 cost= 0.148725004\n",
      "Epoch: 0196 cost= 0.147325464\n",
      "Epoch: 0197 cost= 0.147089384\n",
      "Epoch: 0198 cost= 0.146972910\n",
      "Epoch: 0199 cost= 0.146585053\n",
      "Epoch: 0200 cost= 0.145881170\n",
      " Finished!\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 200\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "# 启动session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())# Initializing OP\n",
    "\n",
    "    # 启动循环开始训练\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # 遍历全部数据集\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # 显示训练中的详细信息\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print( \" Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
